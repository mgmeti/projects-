{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mgmeti/projects-/blob/main/Problem_Statement_Santander_Customer_Satisfaction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#F6E6E2\">\n",
        "\n",
        "# AI 1: ML/AI Basics\n",
        "## Project: Santander Customer Satisfaction\n",
        "\n",
        "**AI 1 Cohort 3**<br/>\n",
        "**Univ.AI**<br/>\n",
        "**Instructor**: Pavlos Protopapas<br />\n",
        "\n",
        "**Team Name:** Stat.<br />\n",
        "**Team Members:**\n",
        "Anush Pittu,\n",
        "Mallikarjun Meti,\n",
        "Ritik Gupta,\n",
        "Sibi Chakravarthy Karunakaran."
      ],
      "metadata": {
        "id": "s7HZGegE4WRr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Links:\n",
        "\n",
        "1. https://www.kaggle.com/c/santander-customer-satisfaction/data\n",
        "2. https://edstem.org/us/courses/14588/lessons/29560/slides/169380"
      ],
      "metadata": {
        "id": "ZCVbL5pdPuOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview:<br/>\n",
        "\n",
        "We will be downloading the Santander Customer Satisfaction dataset from Kaggle. The dataset has two different .csv files - training and testing datasets. We will first explore the training data. During pre-processing, we eliminate  a few features which are not significant to the predict the target class. Then, we experiment with fitting logistc regression and decision tree models and then fit ensemble models. Finally, we will compare the performance of all the models."
      ],
      "metadata": {
        "id": "FHF083CR4ghn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_T8zFvtvPoXj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b73fb6c-da8e-41be-b4a8-871ff279cf4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kaggle\n",
            "  Using cached kaggle-1.5.16-py3-none-any.whl\n",
            "Installing collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.5.16\n",
            "    Uninstalling kaggle-1.5.16:\n",
            "      Successfully uninstalled kaggle-1.5.16\n",
            "Successfully installed kaggle-1.5.16\n"
          ]
        }
      ],
      "source": [
        "#Installing Kaggle API\n",
        "!pip install --upgrade --force-reinstall --no-deps kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1iZTNwxjrCQ1sKpi9p0GejdVPu2ayMyzH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0Rz1RjaQOlv",
        "outputId": "6ea41b00-bef2-4740-c831-7b787cfa581c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1iZTNwxjrCQ1sKpi9p0GejdVPu2ayMyzH\n",
            "To: /content/kaggle.json\n",
            "100% 66.0/66.0 [00:00<00:00, 325kB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "\n",
        "! cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "YkjVjtIxJRZg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69d65e2f-a17a-4d45-b802-1a87b0affd1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "rvbCDjOJTibj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Downloading Santander Customer Satisfaction dataset\n",
        "!kaggle competitions download -c santander-customer-satisfaction"
      ],
      "metadata": {
        "id": "xVONrB4STiYz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65e1f88b-425a-47f9-da27-41635bc60296"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "401 - Unauthorized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Unzipping the dataset\n",
        "!unzip /content/santander-customer-satisfaction.zip"
      ],
      "metadata": {
        "id": "_yp-E2QMTiBm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a41a4abf-dba2-4807-a173-083f651e06f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open /content/santander-customer-satisfaction.zip, /content/santander-customer-satisfaction.zip.zip or /content/santander-customer-satisfaction.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing all necessorry libraries\n",
        "import pandas as pd   # for data analysis and manipulation\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import xgboost as xgb\n",
        "\n",
        "\n",
        "\n",
        "# to sample the hyperparameter space based on distributions\n",
        "from scipy import stats                                                                                                                                                                                                                lol,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,k\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# metrics for performance comparison\n",
        "from sklearn.metrics import accuracy_score\n",
        "#from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# ensemble models for classifiers\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "plt.style.use('seaborn')\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "C6jE9iHSTh7Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72e18bcd-7d44-4e11-9e4a-e7dd0aaca059"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-71af6d20616a>:32: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
            "  plt.style.use('seaborn')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the Santander Customer Satisfaction dataset\n",
        "train_data = pd.read_csv('/content/train.csv')\n",
        "test_data = pd.read_csv('/content/test.csv')\n",
        "\n",
        "# Check number of rows and columns for test and train datasets\n",
        "print(\"Train Data Shape : \",train_data.shape)\n",
        "print(\"Test Data Shape : \",test_data.shape)"
      ],
      "metadata": {
        "id": "NfGkefS0K07U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "ce817961-f6dd-403d-a4c0-d13fea7806c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-01dd30d657b2>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load the Santander Customer Satisfaction dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Check number of rows and columns for test and train datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory Data Analysis<br/>\n",
        "\n",
        "\n",
        "To explore the dataset, we will do the following tasks.<br/>\n",
        "\n",
        "*   Look into the first 5 rows of the dataframe.<br/>\n",
        "*   Observe any naming convention used for column names.<br/>\n",
        "*   Get the summary of the dataframe using .info() <br/>\n",
        "*   Plot some of the variables and try to understand them.<br/>"
      ],
      "metadata": {
        "id": "WIXy-cI24GW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# look into the datasets\n",
        "train_data.head()       # for train dataset\n",
        "test_data.head()        # for test dataset"
      ],
      "metadata": {
        "id": "zsTR0LjYK3Si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:** It looks like most of the features are numerical."
      ],
      "metadata": {
        "id": "QvQ9bV-RAjHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Looking into the column names\n",
        "train_data.columns.values"
      ],
      "metadata": {
        "id": "ADbMkjt2Cqz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:**<br/>\n",
        "we can see three big groups of features:\n",
        "\n",
        "\n",
        "1.   Features starting with imp_, num_, saldo_. Probably from importe (amount), numerico (numerical), and saldo (balance). They should be numerical.\n",
        "2.   Features starting with deltaimp, deltanum. A feature linked and probably calculated based on the previous features. They should be numerical too.\n",
        "\n",
        "3.   Features starting with ind_. Looks like an index (numerical).\n",
        "\n",
        "<br/> Other than the group of features above, these are the other features in our dataset:<br>\n",
        "\n",
        "*   var3<br>\n",
        "*   var15<br>\n",
        "\n",
        "*   var38<br>\n",
        "*   TARGET\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UcOzUEFgDDdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To get an overall summary of the types of the columns in the dataset\n",
        "train_data.info()"
      ],
      "metadata": {
        "id": "NJWlrLPTCBAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**var3:**<br/>\n",
        "According to some Kaggle users, the var3 feature would correspond to the customer country. Let's look into this feature."
      ],
      "metadata": {
        "id": "UEx1TM1MVnSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting value_counts() for var3\n",
        "train_data.var3.value_counts()"
      ],
      "metadata": {
        "id": "NbHdw8onVyOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:**<br/>\n",
        "Looks like -999999 is a placeholder when the country is unkown. We'll replace this with the most common value 2 later."
      ],
      "metadata": {
        "id": "Uaum411qV7zt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# filter by top countries, excluding the most common one (2) and see the distribution\n",
        "top_countries = train_data[(train_data.var3 != -999999) & (train_data.var3 != 2)].groupby('var3').filter(lambda x: len(x) > 80)\n",
        "\n",
        "# plot number of satisfied (0)/ unsatisfied customers by country\n",
        "sns.catplot(x='var3', hue='TARGET', kind='count', data=top_countries)\n"
      ],
      "metadata": {
        "id": "ce3qvYVyWPDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**var15:**<br>\n",
        "According to some kagglers, var15 feature could correspond to the customer age. Let's explore it."
      ],
      "metadata": {
        "id": "xY0EAapgWnSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting value_counts() for var15\n",
        "train_data.var15.value_counts()"
      ],
      "metadata": {
        "id": "vF7UCcBpWsTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:**<br/>\n",
        "It makes sense looking at the data. The most common age (var15) is 23 years. Let's further see how the age relates to the customer satisfaction."
      ],
      "metadata": {
        "id": "uSwsaT9VW1zy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting number of rows of data for var15 (age) less than 23\n",
        "print(train_data[(train_data.var15 < 23)].shape)\n",
        "\n",
        "# Getting the sum of TARGET for var15 (age) less than 23\n",
        "print(train_data[(train_data.var15 < 23)].TARGET.sum())"
      ],
      "metadata": {
        "id": "9pfdVtNDW4I3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:**<br/>\n",
        "There are no unsatisfied customers below 23 years. Let's further look at the satisfaction rate for every age."
      ],
      "metadata": {
        "id": "yIWPR1nOXA2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting TARGET against var15 (age) to look at the satisfaction rate for every age\n",
        "from matplotlib.ticker import PercentFormatter\n",
        "g = sns.catplot(x='var15', y='TARGET', kind='bar', data=train_data[(train_data.var15 > 22) & (train_data.var15 < 100)], aspect=3)\n",
        "\n",
        "for ax in g.axes.flat:\n",
        "    ax.yaxis.set_major_formatter(PercentFormatter())\n",
        "plt.title('TARGET against var15 (age) to look at the satisfaction rate for every age')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zTMXJ-IfXfA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:**<br/>\n",
        "In the age group 23 to 40, the percentage of unsatisfied customers increases as age increases. In the age group 41 to 60, the percentage of unsatisfied customers remains fairly constant with age. In the age group 61 to 78, the percentage of unsatisfied customers decreases with age."
      ],
      "metadata": {
        "id": "3RTXcT2dZkQp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**var38:**<br>\n",
        "According to some users, var38 could correspond to the Mortgage. Let's see."
      ],
      "metadata": {
        "id": "MV8ZoYflZpgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting value_counts() for var38\n",
        "train_data.var38.value_counts()"
      ],
      "metadata": {
        "id": "WViZCScxZmBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting a boxplot for var38\n",
        "plt.boxplot(train_data['var38'])\n",
        "plt.title('Boxplot for var38')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CS2jn4BHpWSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:**<br/>\n",
        "The most common value for var38 is 117310, which according to some users may correspond to the median value of a mortgage in Spain. Let's further explore var38 below."
      ],
      "metadata": {
        "id": "1_YdIeM2Z4i1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting histogram to get the distribution of var38 feature\n",
        "train_data[(train_data.var38 != 117310.979016494) & (train_data.var38 < 300000)].var38.hist(bins=20)"
      ],
      "metadata": {
        "id": "c6oc_ArUbQhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:**<br/>\n",
        "Here we can see that the most common value is actually close to the average of the rest of the mortgages.<br> Let's look at the value of the mortgage of the unsatisfied customers:"
      ],
      "metadata": {
        "id": "TE5-PXtBbj6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting histogram to get the distribution of var38 feature for unsatisfied customers\n",
        "train_data[(train_data.var38 != 117310.979016494) & (train_data.var38 < 300000) & (train_data.TARGET == 1)].var38.hist(bins=20)"
      ],
      "metadata": {
        "id": "GMOt7MOBbubJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:**<br/>\n",
        "var38 follows a similar distribution as above for unsatisfied customers, and we don't see a direct relation between the amount of the mortgage and the customer satisfaction."
      ],
      "metadata": {
        "id": "OR0bJWBRb5tO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TARGET: Customer satisfaction**<br>\n",
        "As mentioned before, our target feature is the customer satisfaction:<br>\n",
        "0 for satisfied customers<br>\n",
        "1 for unsatisfied customers<br>\n",
        "Let's look at the distribution of the classes."
      ],
      "metadata": {
        "id": "LJrFN96WcBHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# happy customers have TARGET==0, unhappy custormers have TARGET==1\n",
        "# A little less then 4% are unhappy => unbalanced dataset\n",
        "pd.DataFrame(train_data['TARGET'].value_counts())\n",
        "\n",
        "fig, ax = plt.subplots(1,2,figsize=(10,6))\n",
        "train_data['TARGET'].value_counts().plot.pie(\n",
        "    explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\n",
        "\n",
        "sns.countplot('TARGET', data=train_data, ax=ax[1])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VDjqkjwPCD0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:**<br/>\n",
        "Less than 4% of our customers are unsatisfied.<br>\n",
        "We will probably need to do some resampling (either upsampling or downsampling) to balance the classes. <br>\n",
        "We'll also need to take this into account while splitting our dataset in train data and test data."
      ],
      "metadata": {
        "id": "duJ2bt5Jcz8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checking multicollinearity between the features\n",
        "features = train_data.drop(['ID','TARGET'],axis=1)\n",
        "\n",
        "features[features.columns[:8]].corr()\n",
        "\n",
        "\n",
        "sns.heatmap(features[features.columns[:8]].corr(),annot=True,cmap='YlGnBu')\n",
        "fig=plt.gcf()\n",
        "fig.set_size_inches(10,8)\n",
        "plt.title('Multicollinearity of top 8 features')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2YeKRR4NCD4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**num_var4:**<br/>\n",
        "According to dmi3kno (see https://www.kaggle.com/cast42/santander-customer-satisfaction/exploring-features/comments#115223),\n",
        "num_var4 could be the number of bank products. Let's plot the distribution for num_var4."
      ],
      "metadata": {
        "id": "5Tkt77NR_Jzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the distribution of num_var4 (number of bank products)\n",
        "train_data.num_var4.hist(bins=100)\n",
        "plt.xlabel('Number of bank products')\n",
        "plt.ylabel('Number of customers in train')\n",
        "plt.title('Most customers have 1 product with the bank')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UF9RQYB8E3dB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's look at the density of the of happy/unhappy customers in function of the number of bank products\n",
        "sns.FacetGrid(train_data, hue=\"TARGET\", size=6) \\\n",
        "   .map(plt.hist, \"num_var4\") \\\n",
        "   .add_legend()\n",
        "plt.title('Unhappy cosutomers have less products')\n",
        "\n",
        "train_data[train_data.TARGET==1].num_var4.hist(bins=6)\n",
        "plt.title('Amount of unhappy customers in function of the number of products')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rWxODjbUFGVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Preprocessing and Cleaning of the data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nzfW9tvwV_DP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# look into the min, max, mean of the features\n",
        "train_data.describe()"
      ],
      "metadata": {
        "id": "KdeMuEj0CCT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check for null values in data set\n",
        "null_value = train_data.isnull().sum()\n",
        "null_value"
      ],
      "metadata": {
        "id": "rZ9jaRoUCC2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:**<br/>\n",
        "There is no null value in the dataset."
      ],
      "metadata": {
        "id": "PjlVCKZQDx6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# drop the 'ID' and 'TARGET' columns from train data set\n",
        "train = train_data.drop(columns=['ID', 'TARGET'], axis=1)\n",
        "test = test_data.drop(columns=['ID'], axis=1)\n",
        "\n",
        "train.shape, test.shape"
      ],
      "metadata": {
        "id": "W0T4GXBvCC9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Constant Features Removal**<br>\n",
        "We remove the features which have zero variance.<br>\n",
        "Constant features provide no information that can help in classification."
      ],
      "metadata": {
        "id": "sdID57one149"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature selector that removes all low-variance features.\n",
        "constant_filter = VarianceThreshold(threshold=0)\n",
        "# Train data set\n",
        "# fit the varaince threshold object\n",
        "constant_filter.fit(train)\n",
        "# transform the train data\n",
        "constant_filter.transform(train)\n",
        "# get_spport() will give boolean output\n",
        "# we get train dataframe by removing constant features\n",
        "train_filter = train[train.columns[constant_filter.get_support()]]\n",
        "# print the shape of the train data set before and after.\n",
        "print('Train dtat set:',train_filter.shape,train.shape)\n",
        "\n",
        "test_filter = test[test.columns[constant_filter.get_support()]]\n",
        "# print the shape of the test data set before and after.\n",
        "print('Test data set:',test_filter.shape,test.shape)\n"
      ],
      "metadata": {
        "id": "Pj-8ymn8CDHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:**<br/>\n",
        "The number of features has been reduced from 369 to 335, which means 34 constant features have been removed.\n"
      ],
      "metadata": {
        "id": "e1bDuGR_f3Vu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quasi Constant Features Removal:**<BR/>\n",
        "Quasi Constant is like constant feature. In constant feature the VarianceThreshold is 0, while in quasi-constant feature  VarianceThreshold is 0.1.\n"
      ],
      "metadata": {
        "id": "XuXJig46hcbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature selector that removes all low-variance features(threshold=0.01)\n",
        "quasi_constant_filter = VarianceThreshold(threshold=0.01)\n",
        "\n",
        "# Train data set\n",
        "# fit the varaince threshold object\n",
        "quasi_constant_filter.fit(train_filter)\n",
        "# transform the train data\n",
        "quasi_constant_filter.transform(train_filter)\n",
        "# get_spport() will give boolean output\n",
        "# we get train dataframe by removing constant features\n",
        "train_quasi_filter = train_filter[train_filter.columns[quasi_constant_filter.get_support()]]\n",
        "# print the shape of the train data set before and after.\n",
        "print('Train data set:',train_quasi_filter.shape, train_filter.shape)\n",
        "\n",
        "\n",
        "# we get test dataframe by removing constant features\n",
        "test_quasi_filter = test_filter[test_filter.columns[quasi_constant_filter.get_support()]]\n",
        "# print the shape of the test data set before and after.\n",
        "print('Test data set:', test_quasi_filter.shape, test_filter.shape)\n"
      ],
      "metadata": {
        "id": "0sJT07mNCDLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:**<br/>\n",
        "After the removal of quasi constant features, the number of features has been reduced from 371 to 272."
      ],
      "metadata": {
        "id": "7GoxQfdTis57"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Duplicate Features Removal:**<br>\n",
        "Sometimes we have duplicate columns.<br>\n",
        "We take the first column and remove the duplicate column."
      ],
      "metadata": {
        "id": "H53YilDHnDBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# take  transpose of filtered train data set  and test data set\n",
        "train_Trans = train_quasi_filter.T\n",
        "test_Trans = test_quasi_filter.T\n",
        "\n",
        "# convert array into dataframe\n",
        "X_train_T = pd.DataFrame(train_Trans)\n",
        "X_test_T = pd.DataFrame(test_Trans)\n",
        "\n",
        "# print the shape of transposed train data\n",
        "print('The shape of transposed train data', X_train_T.shape)\n",
        "\n",
        "# here df.duplicated checks for duplicates along rows( it gives boolean output)\n",
        "duplicated_features = X_train_T.duplicated()\n",
        "\n",
        "# print number of duplicates are there in the data set\n",
        "print('Number of duplicates are ',duplicated_features.sum())\n",
        "\n",
        "# get the binary list as true for keeping feature in dta set by inverting duplicated_features\n",
        "features_to_keep = [not index for index in duplicated_features]\n",
        "\n",
        "# get the unique  data set and transpose back to original state\n",
        "X_train_unique = X_train_T[features_to_keep].T\n",
        "# print the shape of the train data set before and after.\n",
        "print('Train data set:',X_train_unique.shape, train.shape)\n",
        "\n",
        "# get the unique  data set and transpose back to original state\n",
        "X_test_unique = X_test_T[features_to_keep].T\n",
        "# print the shape of the train data set before and after.\n",
        "print('Test data set:',X_test_unique.shape, test.shape)\n"
      ],
      "metadata": {
        "id": "VoTYcXuSCDcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:**<br/>\n",
        "After the removal of duplicate features, the number of features has been reduced from 272 to 255."
      ],
      "metadata": {
        "id": "AW590fZ2Nike"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Correlated Features Removal:**<br>\n",
        "There can be some columns which have high correlation.<br> We will check correlation between two columns i and j and if the correlation is greater than 0.98, we will remove column i from the dataset.<br> We will repeat this exercise for all columns in the dataset."
      ],
      "metadata": {
        "id": "TCi_s3GuNwmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining a correlation function which will return the name of the correlated columns\n",
        "def correlation(dataset, threshold):\n",
        "    col_corr = set()   # Set of all the names of correlated columns\n",
        "    corr_matrix = dataset.corr()   # calculates the correlation\n",
        "    for i in range(len(corr_matrix.columns)):\n",
        "\n",
        "        for j in range(i):\n",
        "            if abs(corr_matrix.iloc[i, j]) > threshold:  # we check absolute coeff value and  threshold\n",
        "                colname = corr_matrix.columns[i]  # getting the name of column out of i, j\n",
        "                col_corr.add(colname)\n",
        "    return col_corr"
      ],
      "metadata": {
        "id": "JxU4T5gSCDjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#getting the name of the correlated columns by calling correlation function (threshold=0.98)\n",
        "corr_features = correlation(X_train_unique, 0.98)\n",
        "\n",
        "# print number of correlated features\n",
        "print('The number of correlated features are :', len(set(corr_features)))\n",
        "\n",
        "# Drop the correlated columns from data set\n",
        "train_clean = X_train_unique.drop(corr_features,axis=1)\n",
        "\n",
        "# Drop the correlated columns from data set\n",
        "test_clean = X_test_unique.drop(corr_features,axis=1)\n",
        "\n",
        "# print the shape of data set after droping the correalated features\n",
        "print('Train data set:',train_clean.shape, train.shape)\n",
        "# print the shape of data set after droping the correalated features\n",
        "print('Train data set:',test_clean.shape, test.shape)\n"
      ],
      "metadata": {
        "id": "Y8NA88aiCDpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace placeholder in the feature var3 (country) with most common term 2\n",
        "train_clean['var3'] = train_clean['var3'].replace(-999999, 2)"
      ],
      "metadata": {
        "id": "0xDzloX7CDru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a copy of train_clean and assigning it to train_df\n",
        "# Modifications to the data or indices of the copy will not be reflected in the\n",
        "# original object\n",
        "train_df  = train_clean.copy()"
      ],
      "metadata": {
        "id": "weLfpBh_CEDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take copy of the test data set\n",
        "x_test = test_clean.copy()\n",
        "\n",
        "y_true = pd.read_csv('/content/sample_submission.csv')\n",
        "print('Y_true has 0 and 1 :',pd.DataFrame(y_true['TARGET'].value_counts()))\n",
        "y_true = y_true['TARGET']\n",
        "\n",
        "# print the shape of test data set\n",
        "print('The shape of the test data',x_test.shape)"
      ],
      "metadata": {
        "id": "VX6F8rXSCELZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# assigning x and y variables\n",
        "# train_df is preprocessed data set (columns ID, TAREGET are dropped)\n",
        "# it has predictores only\n",
        "x = train_df\n",
        "\n",
        "# Response variable\n",
        "y = train_data['TARGET']"
      ],
      "metadata": {
        "id": "D4RstJe2CENa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the x, y as 80% train and 20% for validation, Random_state= 0, stratify by response variable\n",
        "x_train, x_val, y_train, y_val = train_test_split(x,y, test_size = 0.2, random_state = 0, stratify = y)\n",
        "\n",
        "# print the shapes of train, validation after splitting the predictors and response variable\n",
        "print(x_train.shape, x_val.shape, y_train.shape, y_val.shape)"
      ],
      "metadata": {
        "id": "iS3H6Mkk1fzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scaling:**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "we will standarize the features in  training and validation datasets using StandardScaler()<br>\n",
        "We won't standarize the TARGET feature."
      ],
      "metadata": {
        "id": "ZDEkd0ypnV7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# standardizing features to unit variance\n",
        "sc = StandardScaler()\n",
        "# fit the standard scale with x_train\n",
        "sc.fit(x_train)\n",
        "\n",
        "#getting standardized x_train\n",
        "x_train_std = sc.transform(x_train)\n",
        "\n",
        "#getting standardized x_val\n",
        "x_val_std = sc.transform(x_val)\n",
        "\n",
        "#getting standardized x_test\n",
        "x_test_std = sc.transform(x_test)"
      ],
      "metadata": {
        "id": "TQEYDvSPhISF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining a function to evaluate the classifier:**<br>\n",
        "We will define a function to evaluate the classifier performance. <br>We will print the confusion matrix, accuracy, precision, recall, F1 score and roc_auc."
      ],
      "metadata": {
        "id": "O19sTH82TAOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for getting metrices for a classifier with y_test, y_pred and y_pred_proba as arguments\n",
        "def get_clf_eval(y_test, pred = None, pred_proba = None):\n",
        "    # for confusion matrix of true and predicted values\n",
        "    confusion = confusion_matrix(y_test, pred)\n",
        "    # for accuracy of the model\n",
        "    accuracy = accuracy_score(y_test, pred)\n",
        "    # for precision of the model\n",
        "    precision = precision_score(y_test, pred)\n",
        "    # for recall of the model\n",
        "    recall = recall_score(y_test, pred)\n",
        "    # for f1_score of the model\n",
        "    f1 = f1_score(y_test, pred)\n",
        "    # for area under curve of the model\n",
        "    roc_auc = roc_auc_score(y_test, pred_proba)\n",
        "\n",
        "    # print all the metrices for given model\n",
        "    print('Confusion matrix')\n",
        "    print(confusion)\n",
        "    print('Accuracy : {}'.format(np.around(accuracy,4)))\n",
        "    print('Precision: {}'.format(np.around(precision,4)))\n",
        "    print('Recall : {}'.format(np.around(recall,4)))\n",
        "    print('F1 : {}'.format(np.around(f1,4)))\n",
        "    print('ROC_AUC : {}'.format(np.around(roc_auc,4)))"
      ],
      "metadata": {
        "id": "2VcDykmIqlh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining a function to plot the confusion matrix for a given classifier:**<br>\n",
        "Plot the confusion matrix with labels for true positive, true negative, false positive and false negative and get the count and percentage for each group."
      ],
      "metadata": {
        "id": "w1DhRf9cT2LM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for ploting confusion matrix for a classifier\n",
        "def plot_matrix(cf_matrix):\n",
        "    # names of groups\n",
        "    group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
        "\n",
        "    # number of counts for each group by flattening matrix\n",
        "    group_counts = [\"{0:0.0f}\".format(value) for value in cf_matrix.flatten()]\n",
        "\n",
        "    # calculate the percentages of counts for particular group\n",
        "    group_percentages = [\"{0:.2%}\".format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)]\n",
        "\n",
        "    # give labels for each group by combining names, count, percentages\n",
        "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n",
        "\n",
        "    # convert flattened matrix back into the 2X2\n",
        "    labels = np.asarray(labels).reshape(2,2)\n",
        "    # plot the heatmap using sns module\n",
        "    sns.heatmap(cf_matrix, annot=labels, fmt='')"
      ],
      "metadata": {
        "id": "_9nWFNQo3uJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Base Models<br>\n",
        "For the base models, we will implement LogisticRegression and DecisionTreeClassifier and we will compare the performance of the models with each other."
      ],
      "metadata": {
        "id": "u-SOf97bFK5f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  Logistic Regression<br/>\n"
      ],
      "metadata": {
        "id": "TAah_TynUsO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initiating a logistic regression model\n",
        "lr_model = LogisticRegression(max_iter = 10000, random_state=0)\n",
        "\n",
        "# fitting the logistic regression model on x_train_std and y_train\n",
        "lr_model.fit(x_train_std, y_train)\n",
        "\n",
        "# predicting the target class for x_val_std\n",
        "y_pred = lr_model.predict(x_val_std)\n",
        "\n",
        "# predicting the probabilities for the target class using predict_proba\n",
        "y_pred_proba = lr_model.predict_proba(x_val_std)[:,1]\n",
        "\n",
        "# calling the function to evaluate the classifier\n",
        "get_clf_eval(y_val,y_pred,y_pred_proba)\n",
        "\n",
        "# plotting confusion_matrix\n",
        "cf_matrix = confusion_matrix( y_val,y_pred)\n",
        "plot_matrix(cf_matrix)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jM6sfsl9e_KE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RandomizedSearchCV:**<br/>\n",
        "Hyperparameter Tuning<br>\n",
        "We use RandomizedSearchCV to get the best value of C (inverse of regularization strength) for the logistic regression model, to optimize the roc_auc."
      ],
      "metadata": {
        "id": "zgEVlwcAWfbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# references: https://www.kaggle.com/solegalli/customer-satisfaction-with-imbalanced-data\n",
        "\n",
        "# set up the LogisticRegression with default parameters\n",
        "lr = LogisticRegression(random_state=0)\n",
        "\n",
        "# determine the hyperparameter space\n",
        "# make list of c values\n",
        "\n",
        "param_grid ={\"C\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]}\n",
        "\n",
        "# set up the search\n",
        "search = RandomizedSearchCV(\n",
        "    lr,                # the model\n",
        "    param_grid,         # hyperparam space\n",
        "    scoring='roc_auc',   # metric to optimize\n",
        "    cv=5,                 # k-fold cv = 5\n",
        "    n_iter = 60,\n",
        "    random_state=0,       # reproducibility\n",
        "    refit=True,  )\n",
        "\n",
        "# find best hyperparameters by fitting model\n",
        "search.fit(x_train_std, y_train)\n",
        "\n",
        "# the best hyperparameters are stored in an attribute:\n",
        "print('The best hypermater in given hyper space is :', search.best_params_)"
      ],
      "metadata": {
        "id": "AqgKYhoXLOwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting a new confusion matrix for the model with the best C value\n",
        "# predict by using best model\n",
        "y_pred = search.predict(x_val_std)\n",
        "# get the probabilities of predictors\n",
        "y_pred_proba = search.predict_proba(x_val_std)[:,1]\n",
        "# get the metrices of the best model\n",
        "get_clf_eval(y_val,y_pred,y_pred_proba)\n",
        "# plot the confusion matrix by using plot confusion matrix\n",
        "cf_matrix = confusion_matrix( y_val,y_pred)\n",
        "plot_matrix(cf_matrix)"
      ],
      "metadata": {
        "id": "SQzJFI6LLPCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for test data set\n",
        "y_pred = search.predict(x_test_std)\n",
        "y_pred_proba = search.predict_proba(x_test_std)[:,1]\n",
        "# calling the function to evaluate the classifier\n",
        "# get_clf_eval(y_true,y_pred,y_pred_proba)\n",
        "\n",
        "# # plotting confusion_matrix\n",
        "cf_matrix = confusion_matrix( y_true,y_pred)\n",
        "print('accuracy of test data:',accuracy_score(y_true, y_pred))\n",
        "plot_matrix(cf_matrix)"
      ],
      "metadata": {
        "id": "sK3WZw_EsM1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DecisionTreeClassifier\n",
        "Now, let's use DecisionTreeClassifier on the dataset and compare the performance of the model with that of logistic regression model."
      ],
      "metadata": {
        "id": "ddJp7DuH8SIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# take object of decisiontreeclassifier with random state 0 and with default max_depth\n",
        "dt = DecisionTreeClassifier(random_state=0)\n",
        "# fit decision tree with train data\n",
        "dt.fit(x_train_std, y_train)\n",
        "# predict for validation data\n",
        "y_pred = dt.predict(x_val_std)\n",
        "y_pred_proba = dt.predict_proba(x_val_std)[:,1]\n",
        "# get the metrices for decision tree\n",
        "get_clf_eval(y_val,y_pred,y_pred_proba)\n",
        "# plot the confusion matrix\n",
        "cf_matrix = confusion_matrix( y_val,  y_pred)\n",
        "plot_matrix(cf_matrix)"
      ],
      "metadata": {
        "id": "NphbDR0FVVHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RandomizedSearchCV:**<br/>\n",
        "We use RandomizedSearchCV to get the best value of max_depth for the DecisionTreeClassifier model, to optimize the roc_auc."
      ],
      "metadata": {
        "id": "7m5VzVZL8wHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set up the DecisionTreeClassifier with default parameters\n",
        "dt = DecisionTreeClassifier(random_state=0)\n",
        "\n",
        "# determine the hyperparameter space\n",
        "# we use stats to sample from distributions\n",
        "param_grid =dict(max_depth=stats.randint(1, 30))\n",
        "\n",
        "# set up the search\n",
        "search = RandomizedSearchCV(\n",
        "    dt,             # the model\n",
        "    param_grid,     # hyperparam space\n",
        "    scoring='roc_auc',  # metric to optimize\n",
        "    cv=5,\n",
        "    n_iter = 60,\n",
        "    random_state=0, # reproducibility\n",
        "    refit=True)\n",
        "\n",
        "# find best hyperparameters\n",
        "search.fit(x_train_std, y_train)\n",
        "\n",
        "# the best hyperparameters are stored in an attribute:\n",
        "print('The best hypermater in given hyper space is :', search.best_params_)"
      ],
      "metadata": {
        "id": "-dzvnyhCVVNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#getting new y_pred, given the best max_depth value as obtained from the RandomizedSearchCV\n",
        "y_pred = search.predict(x_val_std)\n",
        "y_pred_proba = search.predict_proba(x_val_std)[:,1]\n",
        "# get the matrices for best decision tree\n",
        "get_clf_eval(y_val,y_pred,y_pred_proba)\n",
        "# plot the confusion matrix\n",
        "cf_matrix = confusion_matrix(y_val,y_pred)\n",
        "plot_matrix(cf_matrix)"
      ],
      "metadata": {
        "id": "BFqG0gjvvMIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ensemble models for imbalanced data <br>\n",
        "Having tried logistic regression and decision tree models, now we will implement ensemble models such as GradientBoostingClassifier, BaggingClassifier and RandomForestClassifier and compare the results with the base models."
      ],
      "metadata": {
        "id": "6eF8DQ4ShbA0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GradientBoostingClassifier"
      ],
      "metadata": {
        "id": "d30G3ULRRFLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I use GBM because it usually out-performs other off-the-shelf\n",
        "# classifiers\n",
        "\n",
        "# set up the gradient boosting classifier with default parameters\n",
        "gbm = GradientBoostingClassifier(random_state=0)\n",
        "\n",
        "# determine the hyperparameter space\n",
        "# we use stats to sample from distributions\n",
        "\n",
        "param_grid = dict(\n",
        "    n_estimators=stats.randint(10, 200),\n",
        "    min_samples_split=stats.uniform(0, 1),\n",
        "    max_depth=stats.randint(1, 5),\n",
        "    loss=('deviance', 'exponential'),\n",
        "    )\n",
        "\n",
        "# set up the search\n",
        "search = RandomizedSearchCV(\n",
        "    gbm,\n",
        "    param_grid,\n",
        "    scoring='roc_auc',\n",
        "    cv=2,\n",
        "    n_iter = 5,\n",
        "    random_state=0,\n",
        "    refit=True,\n",
        ")\n",
        "\n",
        "# find best hyperparameters\n",
        "search.fit(x_train_std, y_train)\n",
        "\n",
        "# the best hyperparameters are stored in an attribute:\n",
        "print('The best hyperparmeters are :',search.best_params_)\n",
        "\n",
        "# using the best values of hyperparameters on gbm and plotting confusion matrix\n",
        "y_pred = search.predict(x_val_std)\n",
        "y_pred_proba = search.predict_proba(x_val_std)[:,1]\n",
        "# get the metrices for best gbm model\n",
        "get_clf_eval(y_val,y_pred,y_pred_proba)\n",
        "# plot the confusion matrix\n",
        "cf_matrix = confusion_matrix(y_val,y_pred)\n",
        "plot_matrix(cf_matrix)"
      ],
      "metadata": {
        "id": "fmVoGH4ukQER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for test data set\n",
        "y_pred = search.predict(x_test_std)\n",
        "y_pred_proba = search.predict_proba(x_test_std)[:,1]\n",
        "# calling the function to evaluate the classifier\n",
        "# get_clf_eval(y_true,y_pred,y_pred_proba)\n",
        "\n",
        "# # plotting confusion_matrix\n",
        "cf_matrix = confusion_matrix( y_true,y_pred)\n",
        "print('accuracy of test data:',accuracy_score(y_true, y_pred))\n",
        "plot_matrix(cf_matrix)"
      ],
      "metadata": {
        "id": "iknVQAvisoXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BaggingClassifier"
      ],
      "metadata": {
        "id": "P16SS2WP9L5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use ensemble bagging to improve the performance of the model\n",
        "clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(),n_estimators = 100, max_features = 0.5)\n",
        "\n",
        "# find best hyperparameters\n",
        "clf.fit(x_train_std, y_train)\n",
        "\n",
        "# the best hyperparameters are stored in an attribute:\n",
        "print('The best hyperparmeters are :',search.best_params_)\n",
        "\n",
        "y_pred = clf.predict(x_val_std)\n",
        "y_pred_proba = clf.predict_proba(x_val_std)[:,1]\n",
        "get_clf_eval(y_val,y_pred,y_pred_proba)\n",
        "\n",
        "cf_matrix = confusion_matrix(y_val,y_pred)\n",
        "plot_matrix(cf_matrix)"
      ],
      "metadata": {
        "id": "PtTrN1O6CMeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RandomForestClassifier"
      ],
      "metadata": {
        "id": "rR7i2WUe9U0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a Random Forest classifier all default parameters\n",
        "random_forest = RandomForestClassifier(n_estimators=100, random_state=0)\n",
        "# fit the model\n",
        "random_forest.fit(x_train_std, y_train)\n",
        "\n",
        "y_pred = random_forest.predict(x_val_std)\n",
        "y_pred_proba = random_forest.predict_proba(x_val_std)[:,1]\n",
        "get_clf_eval(y_val,y_pred,y_pred_proba)\n",
        "\n",
        "cf_matrix = confusion_matrix(y_val,y_pred)\n",
        "plot_matrix(cf_matrix)\n"
      ],
      "metadata": {
        "id": "9ZDoezLTr_50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AdaBoostClassifier"
      ],
      "metadata": {
        "id": "74p9eybb8Efv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# used boost ensemble with default parameters\n",
        "clf = AdaBoostClassifier(n_estimators=100, random_state=0).fit(x_train_std, y_train)\n",
        "\n",
        "y_pred = clf.predict(x_val_std)\n",
        "y_pred_proba = clf.predict_proba(x_val_std)[:,1]\n",
        "get_clf_eval(y_val,y_pred,y_pred_proba)\n",
        "\n",
        "cf_matrix = confusion_matrix(y_val,y_pred)\n",
        "plot_matrix(cf_matrix)"
      ],
      "metadata": {
        "id": "a4UiRDaLWbxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### XGBClassifier"
      ],
      "metadata": {
        "id": "I0t7GYfs8Ik4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBooost classifier with default parameters\n",
        "model = XGBClassifier()\n",
        "model.fit(x_train_std, y_train)\n",
        "\n",
        "y_pred = model.predict(x_val_std)\n",
        "y_pred_proba = model.predict_proba(x_val_std)[:,1]\n",
        "get_clf_eval(y_val,y_pred,y_pred_proba)\n",
        "\n",
        "cf_matrix = confusion_matrix(y_val,y_pred)\n",
        "plot_matrix(cf_matrix)"
      ],
      "metadata": {
        "id": "3KUmRb_aYG29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for test data set\n",
        "y_pred = model.predict(x_test_std)\n",
        "y_pred_proba = model.predict_proba(x_test_std)[:,1]\n",
        "# calling the function to evaluate the classifier\n",
        "# get_clf_eval(y_true,y_pred,y_pred_proba)\n",
        "\n",
        "# # plotting confusion_matrix\n",
        "cf_matrix = confusion_matrix( y_true,y_pred)\n",
        "print('accuracy of test data:',accuracy_score(y_true, y_pred))\n",
        "plot_matrix(cf_matrix)"
      ],
      "metadata": {
        "id": "inPw_BluDRIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we used roc_auc as metrice for comparison of models:\n",
        "out all ensembles models:\n",
        "XGBoost is giving roc_auc as .8246\n",
        "\n",
        "we taken thet XGBoost as over best model"
      ],
      "metadata": {
        "id": "E0kV3ULTDhXZ"
      }
    }
  ]
}